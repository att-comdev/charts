# Default values for distribution.
# This is a YAML-formatted file.
# Beware when changing values here. You should know what you are doing!
# Access the values with {{ .Values.key.subkey }}

# Common

images:
  tags:
    initContainerImage: "alpine:3.8"
    distribution: docker.bintray.io/jfrog/distribution-distribution:1.6.1
    distributor: docker.bintray.io/jfrog/distribution-distributor:1.6.1
    redis: docker.io/bitnami/redis:4.0.11-debian-9
    logger: busybox:1.30

# For supporting pulling from private registries
imagePullSecrets:

#for HA
replicaCount: 1

## Role Based Access Control
## Ref: https://kubernetes.io/docs/admin/authorization/rbac/
rbac:
  create: false
  role:
    ## Rules to create. It follows the role specification
    rules:
    - apiGroups:
      - ''
      resources:
      - services
      - endpoints
      - pods
      verbs:
      - get
      - watch
      - list

networkpolicy:
  # Allows all ingress and egress
  - name: distribution
    podSelector:
      matchLabels:
        app: distribution
    egress:
      - {}
    ingress:
      - {}
  # Uncomment to allow only distribution pods to communicate with postgresql (if postgresql.enabled is true)
  # - name: postgres
  #   podSelector:
  #     matchLabels:
  #       app: postgresql
  #   ingress:
  #   - from:
  #     - podSelector:
  #         matchLabels:
  #           app: distribution
  # Uncomment to allow only distribution pods to communicate with redis
  # - name: redis
  #   podSelector:
  #     matchLabels:
  #       app: redis
  #   ingress:
  #   - from:
  #     - podSelector:
  #         matchLabels:
  #           app: distribution

## Service Account
## Ref: https://kubernetes.io/docs/admin/service-accounts-admin/
##
serviceAccount:
  create: false
  ## The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the fullname template
  name:

pod:
  mandatory_access_control:
    type: apparmor
    distribution:
      distributor: runtime/default
      wait-for-db: runtime/default
      redis: runtime/default
      distribution: runtime/default
secrets:
  tls:
    distribution:
      distribution:
        public: distribution-tls-public

endpoints:
  distribution:
    host_fqdn_override:
      default: null
    # NOTE(SaiBattina): this chart supports TLS for fqdn over-ridden public
    # endpoints using the following format:
    #   public:
    #     host: distibution.domain.example
    #     tls:
    #       crt: null
    #       key: null
    #       ca: null

ingress:
  enabled: false
  # Used to create an Ingress record.
  hosts:
    - distribution.domain.example
  annotations:
  # kubernetes.io/ingress.class: nginx
  # kubernetes.io/tls-acme: "true"
  tls:
  # Secrets must be manually created in the namespace.
  # - secretName: distribution-secret-tls
  #   hosts:
  #     - distribution.domain.example

# Sub charts
## Configuration values for the mongodb dependency
## ref: https://github.com/kubernetes/charts/blob/master/stable/mongodb/README.md
##
mongodb:
  enabled: false
  image:
    tag: 3.6.3
    pullPolicy: IfNotPresent
  persistence:
    enabled: false
    size: 10Gi
  resources: {}
  #  requests:
  #    memory: "2Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "250m"
  ## Make sure the --wiredTigerCacheSizeGB is no more than half the memory limit!
  ## This is critical to protect against OOMKill by Kubernetes!
  mongodbExtraFlags:
  - "--wiredTigerCacheSizeGB=1"
  mongodbRootPassword: password
  mongodbUsername: distribution
  mongodbPassword: password
  mongodbDatabase: bintray
  livenessProbe:
    initialDelaySeconds: 40
  readinessProbe:
    initialDelaySeconds: 30

postgresql:
  enabled: true
  imageTag: "9.6.11"
  postgresDatabase: "distribution"
  postgresUser: "distribution"
  postgresPassword: distribution
  postgresConfig:
    maxConnections: "1500"
  service:
    port: 5432
  persistence:
    enabled: true
    size: 50Gi
#    existingClaim:
    storageClass: general
  resources: {}
#    requests:
#      memory: "1Gi"
#      cpu: "250m"
#    limits:
#      memory: "2Gi"
#      cpu: "1"
  nodeSelector: {}
  tolerations: []
  affinity: {}

## If NOT using the PostgreSQL in this chart (postgresql.enabled=false),
## specify custom database details here or leave empty and Artifactory will use embedded derby
database:
  host:
  port:
  database:
  ## If you would like this chart to create the secret containing the db
  ## password, use these values
  user:
  password:
  ## If you have existing Kubernetes secrets containing db credentials, use
  ## these values
  secrets: {}
  #  user:
  #    name: "database-creds"
  #    key: "db-user"
  #  password:
  #    name: "database-creds"
  #    key: "db-password"

## Configuration values for the redis dependency
## ref: https://github.com/helm/charts/blob/master/stable/redis/README.md
##

## Configuration values for the redis dependency
## ref: https://github.com/kubernetes/charts/blob/master/stable/redis/README.md
##
redis:
  name: redis
  image:
    repository: docker.io/bitnami/redis
    tag: 4.0.11-debian-9
    pullPolicy: IfNotPresent
  port: 6379
  password: password
  uid: 1001
  serviceAccount:
    create: true
  disableCommands: "FLUSHDB,FLUSHALL"
  persistence:
    enabled: true
    path: /bitnami/redis/data
    size: 10Gi
    ## A manually managed Persistent Volume and Claim
    ## Requires redis.persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:
    accessMode: ReadWriteOnce
    storageClass: general
  resources: {}
  #  requests:
  #    memory: "256Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "250m"

  nodeSelector: {}
  tolerations: []
  affinity: {}

common:
  uid: 1020
  gid: 1020

# For setting up external services, must pass the connection URL for them
global:
  mongoUrl:
  mongoAuditUrl:
  redisUrl:

distribution:
  replicaCount: 1
  name: distribution
  image:
    imagePullPolicy: IfNotPresent
  internalPort: 8080
  externalPort: 80
  ## Distribution requires a unique master key
  ## You can generate one with the command:
  ## 'openssl rand -hex 32'
  ## Pass it to helm with '--set distribution.masterKey=${MASTER_KEY}'
  ## IMPORTANT: You should NOT use the example masterKey for a production deployment!
  masterKey: BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
  serviceId:
  env:
    artifactoryUrl:
    btServerUrl:
    artifactoryCi1Url:
    artifactoryEdge1Url:
    artifactoryEdge2Url:
    artifactoryEdge3Url:
  service:
    type: ClusterIP
  ## Add custom init containers
  customInitContainers: |
  #  - name: "custom-setup"
  #    image: "{{ .Values.images.tags.initContainerImage }}"
  #    imagePullPolicy: "{{ .Values.distribution.image.pullPolicy }}"
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'touch {{ .Values.distribution.persistence.mountPath }}/example-custom-setup'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.distribution.persistence.mountPath }}"
  #        name: distribution-data

  resources: {}
  #  requests:
  #    memory: "2Gi"
  #    cpu: "500m"
  #  limits:
  #    memory: "4Gi"
  #    cpu: "2"
  ## Control Java options (JAVA_OPTIONS)
  ## IMPORTANT: keep javaOpts.xmx no higher than resources.limits.memory
  javaOpts:
    xms:
    xmx:
  persistence:
    enabled: true
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    accessMode: ReadWriteOnce
    mountPath: "/var/opt/jfrog/distribution"
    size: 50Gi
    ## distribution data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner. (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: general

  # Add any of the loggers to a sidecar if you want to be able to see them with
  # kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - access.log
  # - distribution.log
  # - request.log

  nodeSelector: {}
  tolerations: []
  affinity: {}

distributor:
  replicaCount: 1
  name: distributor
  image:
    imagePullPolicy: IfNotPresent
  resources: {}
  #  requests:
  #    memory: "2Gi"
  #    cpu: "500m"
  #  limits:
  #    memory: "4Gi"
  #    cpu: "2"
  ## Control Java options (JAVA_OPTIONS)
  ## IMPORTANT: keep javaOpts.xmx no higher than resources.limits.memory
  javaOpts:
    xms:
    xmx:
  persistence:
    enabled: true
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    accessMode: ReadWriteOnce
    mountPath: "/var/opt/jfrog/distributor"
    size: 50Gi
    ## distribution data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: general
  # Add any of the loggers to a sidecar if you want to be able to see them with
  # kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - distributor.log
  # - foreman.log

  nodeSelector: {}
  tolerations: []
  affinity: {}
manifests:
  secret_ingress_tls: true
