# Default values for Mission Control.
# This is a YAML-formatted file.
# Beware when changing values here. You should know what you are doing!
# Access the values with {{ .Values.key.subkey }}

# Common
imagePullPolicy: IfNotPresent

uid: 1050
uname: jfmc

imagePullSecrets:

# For HA
replicaCount: 1

images:
  tags:
    initContainerImage: alpine:3.8
    insightExecutor: docker.bintray.io/jfrog/insight-executor:3.5.6
    insightScheduler: docker.bintray.io/jfrog/insight-scheduler:3.5.6
    insightServer: docker.bintray.io/jfrog/insight-server:3.5.6
    missionControl: docker.bintray.io/jfrog/mission-control:3.5.6
    elasticsearch: docker.bintray.io/jfrog/elasticsearch-oss-sg:6.6.0
    esInitContainerImage: alpine:3.6
    logger: busybox:1.30
    postgresql_alpine: postgres:9.6.11-alpine

## Role Based Access Control
## Ref: https://kubernetes.io/docs/admin/authorization/rbac/
rbac:
  create: true
  role:
    ## Rules to create. It follows the role specification
    rules:
    - apiGroups:
      - ''
      resources:
      - services
      - endpoints
      - pods
      verbs:
      - get
      - watch
      - list

networkpolicy:
  # Allows all ingress and egress
  - name: mission-control
    podSelector:
      matchLabels:
        app: mission-control
    egress:
      - {}
    ingress:
      - {}
  # Uncomment to allow only mission-control pods to communicate with postgresql (if postgresql.enabled is true)
  # - name: postgres
  #   podSelector:
  #     matchLabels:
  #       app: postgresql
  #   ingress:
  #   - from:
  #     - podSelector:
  #         matchLabels:
  #           app: mission-control

## Service Account
## Ref: https://kubernetes.io/docs/admin/service-accounts-admin/
##
serviceAccount:
  create: true
  ## The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the fullname template
  name:
pod:
  mandatory_access_control:
    type: apparmor
    mission-control:
      mission-control: runtime/default
      wait-for-db: runtime/default
      prepare-storage: runtime/default
      set-password: runtime/default
# Init containers
initContainers:
  resources:
    requests:
      memory: "64Mi"
      cpu: "10m"
    limits:
      memory: "128Mi"
      cpu: "250m"

# PostgreSQL
## Configuration values for the postgresql dependency
## ref: https://github.com/kubernetes/charts/blob/master/stable/postgresql/README.md
##
postgresql:
  enabled: true
  postgresUsername: postgres
  postgresPassword: postgres
  postgresConfig:
    maxConnections: "1500"
  db:
    name: mission_control
    sslmode: "false"
    tablespace: "pg_default"
    jfmcUsername: jfmc
    jfisUsername: jfis
    jfscUsername: jfsc
    jfexUsername: jfex
    jfmcPassword: jfmc
    jfisPassword: jfis
    jfscPassword: jfsc
    jfexPassword: jfex
    jfmcSchema: jfmc_server
    jfisSchema: insight_server
    jfscSchema: insight_scheduler
    jfexSchema: insight_executor
  service:
    port: 5432
  persistence:
    enabled: true
    size: 50Gi
    existingClaim:
  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "250m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"
  nodeSelector: {}
  affinity: {}
  tolerations: []

### If NOT using the PostgreSQL in this chart (postgresql.enabled=false),
## specify custom database details here or leave empty
database:
  type: postgresql
  host:
  port:
  ## Please make sure these are created under the provided database
  name: mission_control
  jfisSchema: insight_server
  jfmcSchema: jfmc_server
  jfscSchema: insight_scheduler
  jfexSchema: insight_executor
  ## If you would like to use single user and password for all the services
  user:
  password:
  ## If you have existing Kubernetes secrets containing db credentials, use
  ## these values
  secrets: {}
  #  user:
  #    name: "database-creds"
  #    key: "db-user"
  #  password:
  #    name: "database-creds"
  #    key: "db-password"
  ## If you want to use different credentials for each service
  ## Mission-Control
  jfmcUsername: jfmc
  jfmcPassword: jfmc
  ## Insight-Server
  jfisUsername: jfis
  jfisPassword: jfis
  ## Insight-Scheduler
  jfscUsername: jfsc
  jfscPassword: jfsc
  ## Insight-Executor
  jfexUsername: jfex
  jfexPassword: jfex

elasticsearch:
  enabled: true
  name: elasticsearch
  image:
    pullPolicy: IfNotPresent
  ## Enter elasticsearch connection details
  url: http://localhost:9200
  httpPort: 9200
  transportPort: 9300
  username: "admin"
  password: "admin"
  env:
    clusterName: "es-cluster"
    networkHost: "0.0.0.0"
    transportHost: "0.0.0.0"
    maxMapCount: 262144
    minimumMasterNodes: 1

  persistence:
    enabled: true
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    mountPath: "/usr/share/elasticsearch/data"
    accessMode: ReadWriteOnce
    size: 100Gi
    ## ElasticSearch data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: general
  ## ElasticSearch xms and xmx should be same!
  javaOpts: {}
  #  xms: "2g"
  #  xmx: "2g"

  resources: {}
  #  requests:
  #    memory: "2Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2.5Gi"
  #    cpu: "500m"

podRestartTime:

secrets:
  tls:
    missioncontrol:
      missioncontrol:
        public: missioncontrol-tls-public

endpoints:
  cluster_domain_suffix: cluster.local
  missioncontrol:
    host_fqdn_override:
      default: null
    # NOTE(SaiBattina): this chart supports TLS for fqdn over-ridden public
    # endpoints using the following format:
    #   public:
    #     host: mission.domain.example
    #     tls:
    #       crt: null
    #       key: null
    #       ca: null

ingress:
  enabled: false
  defaultBackend:
    enabled: true
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  path: /
  hosts: []
  tls: []
  #  - secretName: missioncontrol-secret-tls
  #    hosts:
  #      - mission.domain.example
  additionalRules: []

missionControl:
  name: mission-control
  appName: jfmc-server
  home: /var/opt/jfrog/mission-control
  # Set this to true if you want to override credentials in mission-control.properties on startup
  propertyOverride: true
  ## Note that by default we use appVersion to get image tag
  # version:

  ## Add custom init containers
  customInitContainers: |
  #  - name: "custom-setup"
  #    image: "{{ .Values.images.tags.initContainerImage }}"
  #    imagePullPolicy: "{{ .Values.imagePullPolicy }}"
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'touch {{ .Values.missionControl.persistence.mountPath }}/example-custom-setup'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.missionControl.persistence.mountPath }}"
  #        name: mission-control-data

  ## Add custom volumes
  customVolumes: |
  #  - name: custom-script
  #    configMap:
  #      name: custom-script

  ## Add custom volumeMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: "/scripts/script.sh"
  #    subPath: script.sh

  ## Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - audit.log
  # - http.log
  # - init-debug.log
  # - init-err.log
  # - init.log
  # - jfmc-server-debug.log
  # - jfmc-server-err.log
  # - jfmc-server.log
  # - monitoring.log
    # Loggers containers resources
  loggersResources: {}
  #  requests:
  #    memory: "64Mi"
  #    cpu: "25m"
  #  limits:
  #    memory: "128Mi"
  #    cpu: "50m"
  ## Mission Control requires a unique mc.key
  # This will be used to encript sensitive data to be stored in database
  ## You can generate one with the command:
  ## 'openssl rand -hex 16'
  ## Pass it to helm with '--set missionControl.mcKey=${MC_KEY}'
  ## IMPORTANT: You should NOT use the example mcKey for a production deployment!
  mcKey: bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb

  missionControlUrl:
  podRestartTime:
  repository: jfrog-mission-control
  package: mc-docker-installer
  dist: helm
  osVersion: "NA"
  osType: "NA"
  osDist: "NA"

  persistence:
    enabled: true
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    mountPath: "/var/opt/jfrog/mission-control"
    accessMode: ReadWriteOnce
    size: 100Gi
    ## Mission Control data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: general

  ## Control Java options (JAVA_OPTIONS)
  ## IMPORTANT: keep resources.limits.memory higher than javaOpts.xmx by 0.5G
  javaOpts:
    other: "-server -XX:+UseG1GC -Dfile.encoding=UTF8"
  #  xms: "2g"
  #  xmx: "3g"
  resources: {}
  #  requests:
  #    memory: "2Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "3.5Gi"
  #    cpu: "1"
  nodeSelector: {}

  tolerations: []

  affinity: {}

  service:
    type: ClusterIP
    annotations: {}
  internalPort: 8080
  externalPort: 80
  ## Extra post-start command to run extra commands after container starts
  # preStartCommand:
  ## For example, to import trusted keystore ( Need to mount the certificates first )
  # preStartCommand: "/java/jdk-11.0.2+9/bin/keytool -importcert -keystore /java/jdk-11.0.2+9/lib/security/cacerts -storepass changeit -file /tmp/trusted-certificates/root.crt -alias 'newcerts'"

insightServer:
  name: insight-server
  home: /opt/jfrog
  internalHttpPort: 8082
  ## This can be used to whitelist the range of IPs allowed to be served by Insight Server service
  ## The value must follow CIDR format
  allowIP: "0.0.0.0/0"
  resources: {}
  #  requests:
  #    memory: "500Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "1Gi"
  #    cpu: "1"

  # Persistence for the logs
  persistence:
    enabled: true
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    mountPath: "/opt/jfrog/insight-server/logs"
    accessMode: ReadWriteOnce
    size: 100Gi
    ## Insight Server logs Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: general

  ## Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - insight-server.log
  # Loggers containers resources
  loggersResources: {}
  #  requests:
  #    memory: "64Mi"
  #    cpu: "25m"
  #  limits:
  #    memory: "128Mi"
  #    cpu: "50m"

insightScheduler:
  name: insight-scheduler
  home: /opt/jfrog
  internalPort: 8085

  ## Control Java options (JFMC_EXTRA_JAVA_OPTS)
  ## IMPORTANT: keep resources.limits.memory higher than javaOpts.xmx by 0.5G
  javaOpts: {}
  #  other:
  #  xms: "500m"
  #  xmx: "1g"
  resources: {}
  #  requests:
  #    memory: "500Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "1.5Gi"
  #    cpu: "1"

  # Persistence for the logs
  persistence:
    enabled: true
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    mountPath: "/opt/jfrog/insight-scheduler/logs"
    accessMode: ReadWriteOnce
    size: 100Gi
    ## Insight Scheduler logs Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: general
  ## Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - insight-scheduler.log
  # - access.log

  # Loggers containers resources
  loggersResources: {}
  #  requests:
  #    memory: "64Mi"
  #    cpu: "25m"
  #  limits:
  #    memory: "128Mi"
  #    cpu: "50m"
  #
insightExecutor:
  name: insight-executor
  home: /opt/jfrog
  image: docker.bintray.io/jfrog/insight-executor
  internalPort: 8087

  ## Control Java options (JFMC_EXTRA_JAVA_OPTS)
  ## IMPORTANT: keep resources.limits.memory higher than javaOpts.xmx by 0.5G
  javaOpts: {}
  #  other:
  #  xms: "500m"
  #  xmx: "2g"
  resources: {}
  #  requests:
  #    memory: "500Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2.5Gi"
  #    cpu: "1"

  # Persistence for the logs
  persistence:
    enabled: true
    ## existingClaim: jfmc-exec-pvc
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    mountPath: "/opt/jfrog/insight-executor/logs"
    accessMode: ReadWriteOnce
    size: 100Gi
    ## Insight Executor logs Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: general

  ## Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - insight-executor.log
  # - access.log

    # Loggers containers resources
  loggersResources: {}
  #  requests:
  #    memory: "64Mi"
  #    cpu: "25m"
  #  limits:
  #    memory: "128Mi"
  #    cpu: "50m"

manifests:
  secret_ingress_tls: true
